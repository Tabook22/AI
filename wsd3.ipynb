{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIygeaFIOGZwQVNOqFTdni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tabook22/AI/blob/main/wsd3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myuJz3zemvY3",
        "outputId": "98c9d555-6e6f-41fa-87a0-6c84450cee17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk scikit-learn ftfy tensorflow keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to replace 'training_data.json', 'testing_data.json', and 'wsd_dictionary.json' with the actual file paths of your JSON files containing the respective data.\n",
        "This code does the following:\n",
        "\n",
        "It loads the training data, testing data, and WSD dictionary from JSON files.\n",
        "It preprocesses the data by extracting the context sentences and lemma IDs.\n",
        "It creates a dictionary mapping lemma IDs to their possible glosses using the WSD dictionary.\n",
        "It defines a function extract_features() that tokenizes the context sentences, converts them to lowercase, removes stopwords, and joins the remaining tokens back into a string. This function is used to extract features from the context sentences.\n",
        "It prepares the training data by extracting features from the training contexts and storing the corresponding lemma IDs.\n",
        "It creates a TF-IDF vectorizer to convert the training contexts into a matrix of TF-IDF features.\n",
        "It trains a Linear SVM model using the training features and lemma IDs.\n",
        "It evaluates the trained model on the testing data by extracting features from the testing contexts, predicting the lemma IDs using the model, and comparing the predicted lemma IDs with the true lemma IDs.\n",
        "Finally, it calculates and prints the accuracy of the WSD model on the testing data.\n",
        "\n",
        "Note that this is a basic implementation and can be further enhanced by incorporating more advanced features, experimenting with different machine learning algorithms, and fine-tuning hyperparameters.\n",
        "Remember to provide the necessary JSON files with the training data, testing data, and WSD dictionary in the specified format for the code to run successfully."
      ],
      "metadata": {
        "id": "Xo8wuFBpnhNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from ftfy import fix_text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7qXf35Xnyq-",
        "outputId": "bb0d7b26-753a-45fe-d456-f00d297687f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data, testing data, and WSD dictionary from JSON files\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        json_text = f.read()\n",
        "        fixed_json_text = fix_text(json_text)\n",
        "        data = json.loads(fixed_json_text)\n",
        "    return data\n",
        "\n",
        "training_data = load_json_file('train.json')\n",
        "testing_data = load_json_file('test_wsd.json')\n",
        "wsd_dictionary = load_json_file('WSD_dict.json')"
      ],
      "metadata": {
        "id": "ghCLwuHIn99C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(data):\n",
        "    preprocessed_data = []\n",
        "    for item in data:\n",
        "        context = item['context']\n",
        "        lemma_id = item['lemma_id']\n",
        "        preprocessed_data.append((context, lemma_id))\n",
        "    return preprocessed_data\n",
        "\n",
        "training_data = preprocess_data(training_data)\n",
        "testing_data = preprocess_data(testing_data)"
      ],
      "metadata": {
        "id": "2fmwKIN1oMYa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary mapping lemma IDs to their possible glosses\n",
        "lemma_to_glosses = {}\n",
        "for item in wsd_dictionary:\n",
        "    lemma_id = item['lemma_id']\n",
        "    gloss = item['gloss']\n",
        "    if lemma_id not in lemma_to_glosses:\n",
        "        lemma_to_glosses[lemma_id] = []\n",
        "    lemma_to_glosses[lemma_id].append(gloss)"
      ],
      "metadata": {
        "id": "cQ6zk3E0oQwi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the context sentences\n",
        "def extract_features(context):\n",
        "    tokens = nltk.word_tokenize(context)\n",
        "    lowercase_tokens = [token.lower() for token in tokens]\n",
        "    filtered_tokens = [token for token in lowercase_tokens if token not in stopwords.words('arabic')]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "_y1glX8poVr1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data\n",
        "train_contexts = [extract_features(context) for context, _ in training_data]\n",
        "train_lemma_ids = [lemma_id for _, lemma_id in training_data]"
      ],
      "metadata": {
        "id": "h8zoTDgKopVC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TF-IDF vectorizer and transform the training contexts\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_features = vectorizer.fit_transform(train_contexts)"
      ],
      "metadata": {
        "id": "pduZgUUPoqdj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad the training contexts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_contexts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_contexts)\n",
        "max_length = max(len(seq) for seq in train_sequences)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "9B-sszicotzW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the testing data\n",
        "test_contexts = [extract_features(context) for context, _ in testing_data]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_contexts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "Te1_ZJhlo1Fm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BiLSTM model\n",
        "bilstm_model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(len(lemma_to_glosses), activation='softmax')\n",
        "])\n",
        "bilstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "TfhogOfm-DYC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BiGRU model\n",
        "bigru_model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length),\n",
        "    Bidirectional(GRU(64)),\n",
        "    Dense(len(lemma_to_glosses), activation='softmax')\n",
        "])\n",
        "bigru_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "JPW8WokM-Kc3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate different models\n",
        "models = [\n",
        "    ('Linear SVM', LinearSVC()),\n",
        "    ('Decision Tree', DecisionTreeClassifier()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('Neural Network', MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500)),\n",
        "    ('BiLSTM', bilstm_model),\n",
        "    ('BiGRU', bigru_model)\n",
        "]\n",
        "\n",
        "for model_name, model in models:\n",
        "    if model_name in ['BiLSTM', 'BiGRU']:\n",
        "        # Train the Keras model\n",
        "        model.fit(train_padded, train_lemma_ids, epochs=5, batch_size=32, validation_split=0.2)\n",
        "    else:\n",
        "        # Train the scikit-learn model\n",
        "        model.fit(train_features, train_lemma_ids)\n",
        "\n",
        "        # Evaluate the model using cross-validation\n",
        "        cv_scores = cross_val_score(model, train_features, train_lemma_ids, cv=5)\n",
        "\n",
        "        print(f\"{model_name} - Cross-validation scores: {cv_scores}\")\n",
        "        print(f\"{model_name} - Average cross-validation score: {cv_scores.mean():.2f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNw7LUKp-Ovk",
        "outputId": "cbd639f6-5cc0-4183-dc35-9634a6228cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model based on cross-validation scores (excluding BiLSTM and BiGRU)\n",
        "best_model = max(models[:-2], key=lambda x: cross_val_score(x[1], train_features, train_lemma_ids, cv=5).mean())\n",
        "best_model_name, best_model = best_model\n",
        "\n",
        "print(f\"Best model: {best_model_name}\")\n"
      ],
      "metadata": {
        "id": "VN_VfSns-UAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the testing data\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for context, true_lemma_id in testing_data:\n",
        "    if best_model_name in ['BiLSTM', 'BiGRU']:\n",
        "        # Prepare the testing context for the Keras model\n",
        "        test_sequence = tokenizer.texts_to_sequences([extract_features(context)])\n",
        "        test_padded = pad_sequences(test_sequence, maxlen=max_length)\n",
        "        predicted_lemma_id = best_model.predict(test_padded).argmax()\n",
        "    else:\n",
        "        # Prepare the testing context for the scikit-learn model\n",
        "        context_features = vectorizer.transform([extract_features(context)])\n",
        "        predicted_lemma_id = best_model.predict(context_features)[0]\n",
        "\n",
        "    if predicted_lemma_id == true_lemma_id:\n",
        "        correct_predictions += 1\n",
        "    total_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Accuracy of the best model on testing data: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "PdCOSRWt-Xyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}